{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to evaluate performance of the models. [As described](https://github.com/s-nlp/detox/tree/main#evaluation) in the [code of the given paper](https://github.com/s-nlp/detox/blob/main/emnlp2021/metric/metric.py), many different metrics are computing to access quality of the models. For simplicity, I will choose only some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metric calculation funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def style_transfer_accuracy(preds, batch_size=32):\n",
    "    print('Calculating style of predictions')\n",
    "    results = []\n",
    "\n",
    "    classifier_model_name = 's-nlp/roberta_toxicity_classifier'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(classifier_model_name)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(classifier_model_name)\n",
    "\n",
    "    for i in tqdm(range(0, len(preds), batch_size)):\n",
    "        batch = tokenizer(preds[i:i + batch_size], return_tensors='pt', padding=True)\n",
    "        with torch.inference_mode():\n",
    "            logits = model(**batch).logits\n",
    "        result = torch.softmax(logits, -1)[:, 1].cpu().numpy()\n",
    "        results.extend([1 - item for item in result])\n",
    "\n",
    "    accuracy = np.mean(results)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def bleu_score(inputs, preds):\n",
    "    bleu_sim = 0\n",
    "    counter = 0\n",
    "    print('Calculating BLEU similarity')\n",
    "    for i in tqdm(range(len(inputs))):\n",
    "        if len(inputs[i]) > 3 and len(preds[i]) > 3:\n",
    "            bleu_sim += sentence_bleu([inputs[i]], preds[i])\n",
    "            counter += 1\n",
    "\n",
    "    return float(bleu_sim / counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style transfer accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sta = style_transfer_accuracy(['you are amazing'])\n",
    "print(f'Style transfer accuracy: {sta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1197.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlEU score: 0.4758733096412523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_score(['you are fuck'], ['you are amazing'])\n",
    "print(f'BlEU score: {bleu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models' results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57809</th>\n",
       "      <td>Listen, call off the butchers, and I'll tell you.</td>\n",
       "      <td>Call your butchers and I'll inform you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132693</th>\n",
       "      <td>who the hell was going through my stuff?</td>\n",
       "      <td>Who has been going through my things?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254505</th>\n",
       "      <td>She still might die . . .?</td>\n",
       "      <td>He may still pass away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451186</th>\n",
       "      <td>that's what his name was.</td>\n",
       "      <td>That's his name.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191213</th>\n",
       "      <td>I'd take you on your shoulders... I'd tie you ...</td>\n",
       "      <td>I would carry you on my back, and we'll journe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   inputs  \\\n",
       "57809   Listen, call off the butchers, and I'll tell you.   \n",
       "132693           who the hell was going through my stuff?   \n",
       "254505                         She still might die . . .?   \n",
       "451186                          that's what his name was.   \n",
       "191213  I'd take you on your shoulders... I'd tie you ...   \n",
       "\n",
       "                                                    preds  \n",
       "57809             Call your butchers and I'll inform you.  \n",
       "132693              Who has been going through my things?  \n",
       "254505                            He may still pass away.  \n",
       "451186                                   That's his name.  \n",
       "191213  I would carry you on my back, and we'll journe...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "llm_result_df_path = '../data/interim/model-outputs/llm-mistral-10shots.csv'\n",
    "llm_result_df = pd.read_csv(llm_result_df_path, index_col=0)\n",
    "llm_result_df.fillna('', inplace=True)  # Fill nan if exist\n",
    "llm_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2/2 [00:12<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/144 [00:00<?, ?it/s]/root/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/root/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/root/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 144/144 [00:00<00:00, 2570.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Style transfer accuracy: 0.8257869377947473\n",
      "BlEU score: 0.3190431760968685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = list(llm_result_df['inputs'])\n",
    "preds = list(llm_result_df['preds'])\n",
    "sta = style_transfer_accuracy(preds, batch_size=128)\n",
    "bleu = bleu_score(inputs, preds)\n",
    "print()\n",
    "print(f'Style transfer accuracy: {sta}')\n",
    "print(f'BlEU score: {bleu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
