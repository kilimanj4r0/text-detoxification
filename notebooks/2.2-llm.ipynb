{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the power of LLMs. We choose for experiment huge models like `mistral`, `llama2`. They should be uncencored. Also, for convenience we will use [LangChain](https://python.langchain.com/docs/get_started/introduction) and [Ollama](https://ollama.ai/) frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>0.014195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0.065473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.213313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.053362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.009402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0  if Alkar floods her with her mental waste, it ...   \n",
       "1                        you're becoming disgusting.   \n",
       "2                      well, we can spare your life.   \n",
       "3                       monkey, you have to wake up.   \n",
       "4                         I have orders to kill her.   \n",
       "\n",
       "                                         translation  similarity  lenght_diff  \\\n",
       "0  If Alkar is flooding her with psychic waste, t...    0.785171     0.010309   \n",
       "1                          Now you're getting nasty.    0.749687     0.071429   \n",
       "2           Well, we could spare your life, for one.    0.919051     0.268293   \n",
       "3          Ah! Monkey, you've got to snap out of it.    0.664333     0.309524   \n",
       "4                   I've got orders to put her down.    0.726639     0.181818   \n",
       "\n",
       "    ref_tox   trn_tox  \n",
       "0  0.981983  0.014195  \n",
       "1  0.999039  0.065473  \n",
       "2  0.985068  0.213313  \n",
       "3  0.994215  0.053362  \n",
       "4  0.999348  0.009402  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# The data should be downloaded and preprocessed, use 1.0-download-raw-data.ipynb and 1.2-data-preprocessing.ipynb notebooks\n",
    "data_path = '../data/internal/preprocessed_filtered.csv'\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting using LangChain framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "\n",
    "set_verbose(True)\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reference': \"Vauxhall, however, has just introduced a new fast Astra and says it isn't terrible rubbish.\",\n",
       "  'translation': 'however, Vauxhall has given a new quick Astra, and says it is not terrible.'},\n",
       " {'reference': 'I forgot the pig!', 'translation': 'I forgot the duster!'},\n",
       " {'reference': 'He broke his neck trying to suck his own dick.',\n",
       "  'translation': 'he broke his neck trying to smoke him.'},\n",
       " {'reference': \"Now the whole damn town knows you're here!\",\n",
       "  'translation': 'so he knows the whole city.'},\n",
       " {'reference': \"Nigger give up quick, didn't he?\",\n",
       "  'translation': 'wrapped up fast, huh?'},\n",
       " {'reference': 'like every year, completely useless!',\n",
       "  'translation': \"Like, every year, it's like, so wasted!\"},\n",
       " {'reference': \"God damn, son! After that, Mama went to the hotel to lay down, so I went out for a walk to see our nation's capital.\",\n",
       "  'translation': 'then my mom went to bed at the hotel, and I went for a walk on our capital.'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 7\n",
    "sampled = df.sample(n=n_samples)\n",
    "zipped = zip(sampled['reference'], sampled['translation'])\n",
    "\n",
    "examples = []\n",
    "for ref, trn in zipped:\n",
    "    examples.append({\n",
    "        'reference': ref,\n",
    "        'translation': trn\n",
    "    })\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "\n",
    "# 1. https://ollama.ai/download\n",
    "# 2. ollama serve\n",
    "# 3. ollama pull mistral\n",
    "llm = Ollama(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "Toxic text: \"{reference}\"\n",
    "Non-toxic text: \"{translation}\"\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "   input_variables=[\"reference\", \"translation\"],\n",
    "   template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"\n",
    "Make the text NON-TOXIC according to the examples below. Write ONLY Non-toxic text as an output! It could not be no response.\\n\n",
    "\"\"\"\n",
    "suffix = \"\"\"\n",
    "Toxic text: \"{reference}\"\n",
    "Non-toxic text: \"\"\"\n",
    "\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "   examples=examples,\n",
    "   example_prompt=example_prompt,\n",
    "   prefix=prefix,\n",
    "   suffix=suffix,\n",
    "   input_variables=[\"reference\"],\n",
    "   example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_llm_chain = LLMChain(\n",
    "   prompt=few_shot_prompt_template,\n",
    "   llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_llm_output(llm_output):\n",
    "    return re.sub(r'.*Non-toxic text: ', '', llm_output).strip()  # TODO: make it better so it always will return smth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304773</th>\n",
       "      <td>sometimes you need to escape, blow out the cit...</td>\n",
       "      <td>Sometimes you need to get away Blow the town a...</td>\n",
       "      <td>0.847671</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483524</th>\n",
       "      <td>You can still enjoy it... before it collapses ...</td>\n",
       "      <td>you still have time to see his beauty... befor...</td>\n",
       "      <td>0.616908</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.002965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43910</th>\n",
       "      <td>The tree, which under its green branches, you ...</td>\n",
       "      <td>a tree under whose green branches you first lo...</td>\n",
       "      <td>0.777978</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.994834</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478287</th>\n",
       "      <td>Damn it.</td>\n",
       "      <td>bloody hell.</td>\n",
       "      <td>0.808032</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.999285</td>\n",
       "      <td>0.485974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15388</th>\n",
       "      <td>Known her since school, first kiss, all that f...</td>\n",
       "      <td>I knew her from school, first kiss and everyth...</td>\n",
       "      <td>0.744810</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.999629</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "304773  sometimes you need to escape, blow out the cit...   \n",
       "483524  You can still enjoy it... before it collapses ...   \n",
       "43910   The tree, which under its green branches, you ...   \n",
       "478287                                           Damn it.   \n",
       "15388   Known her since school, first kiss, all that f...   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "304773  Sometimes you need to get away Blow the town a...    0.847671   \n",
       "483524  you still have time to see his beauty... befor...    0.616908   \n",
       "43910   a tree under whose green branches you first lo...    0.777978   \n",
       "478287                                       bloody hell.    0.808032   \n",
       "15388   I knew her from school, first kiss and everyth...    0.744810   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "304773     0.218750  0.998476  0.000051  \n",
       "483524     0.246575  0.999479  0.002965  \n",
       "43910      0.197917  0.994834  0.000102  \n",
       "478287     0.307692  0.999285  0.485974  \n",
       "15388      0.135593  0.999629  0.000076  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 20\n",
    "sampled_test = df.sample(n=n_samples)\n",
    "zipped_test = zip(sampled_test['reference'], sampled_test['translation'])\n",
    "sampled_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sometimes you need to escape, blow out the city and avoid charting where the night is and day is day, and just sit on your ass.\n",
      "Sometimes you need to get away Blow the town and flee the fray Where night is night, and day is day\n",
      "Sometimes, it's good to take a break, leave the city for a bit, and just relax.\n",
      "===\n",
      "You can still enjoy it... before it collapses in shit.\n",
      "you still have time to see his beauty... before he collapses in disgust.\n",
      "You can still enjoy it... before it collapses in rubbish.\n",
      "===\n",
      "The tree, which under its green branches, you first fucked your girlfriend, entrenched by love.\n",
      "a tree under whose green branches you first loved each other with your girl.\n",
      "The tree, which has green leaves and is where you first made love to your girlfriend, stands tall.\n",
      "===\n",
      "Damn it.\n",
      "bloody hell.\n",
      "\"\n",
      "===\n",
      "Known her since school, first kiss, all that fucking shit.\n",
      "I knew her from school, first kiss and everything.\n",
      "\"Known her since school, first kiss, all that stuff.\"\n",
      "===\n",
      "Mother flip. My jaw must be broken.\n",
      "one of them broke my jaw.\n",
      "\"Mommy, my jaw seems to be injured.\"\n",
      "===\n",
      "I found there was too much improvising on his wretched harmonica.\n",
      "there was too much improvisation on that lousy harmonica.\n",
      "I found there was too much improvising on his wretched harmonica.\n",
      "===\n",
      "You are a good girl... ...but you are cursed in love.\n",
      "you're a good girl, but you're too bad for love.\n",
      "\"You are a wonderful person... ...but I'm worried about your love life.\"\n",
      "===\n",
      "You guys have every corrupt cop in Rio on your tail.\n",
      "you have all the undercover cops from Rio behind your back.\n",
      "\"You guys have every corrupt law enforcement officer in Rio following you around.\"\n",
      "===\n",
      "you made her a monster.\n",
      "You've really created a monster there.\n",
      "\"you made her a nice person.\"\n",
      "===\n",
      "I killed Francis.\n",
      "I ki... : ...killed Francis.\n",
      "\"I killed the flower.\"\n",
      "===\n",
      "not even after Gary hired someone to kill you both?\n",
      "Even after Gary supposedly hired a hit man to kill both of you? I'm sorry?\n",
      "\"not even after Gary tried to hire someone to kill you?\"\n",
      "===\n",
      "would you die for me?\n",
      "Would you die for me? Yes.\n",
      "Would you live for me?\"\n",
      "===\n",
      "now you have to show up, and you know you're fake and you dance like a bear.\n",
      "Now you've got to perform and you know your voice is off and your dancing clumsy.\n",
      "Now you need to appear, and you are aware that you are dishonest and move like a bear.\n",
      "===\n",
      "From a soccer team to a freakin' deer carcass.\n",
      "from the football team to the deer carcass.\n",
      "From a sporting club to a beautiful forest animal.\n",
      "===\n",
      "all I can think about is the useless planet.\n",
      "All I can think of is that waste of a planet.\n",
      "\"all I can think about is the wasted earth.\"\n",
      "===\n",
      "The lurid descriptions in such texts would be almost humorous, were it not for the unhappy fact that even though contemporary sexologists, doctors, writers, etc. have done much to remove the stigma of masturbation, the deep-seated guilts induced by the nonsense in those sexual primers have been only partially erased.\n",
      "the repulsive descriptions in these manuals could be almost amusing if it wasn't for the sad fact that, despite the efforts of current sexuologists, doctors, writers and others, to remove the stigma that masturbation carried, it managed deeply rooted feelings of guilt, triggered by the nonsense in these sexual manuals, to be erased only in part.\n",
      "\"The detailed descriptions contained within these texts are somewhat amusing, despite the unfortunate reality that even though modern sexologists, doctors, authors, and others have made significant efforts to eliminate the shame associated with masturbation, the deep-rooted anxieties triggered by the senseless content in such sexual guides remain only partially alleviated.\"\n",
      "===\n",
      "you're quite an antisocial.\n",
      "You're kind of antisocial, you know that?\n",
      "\"You seem quite introverted.\"\n",
      "===\n",
      "It's called \"go to hell.\"\n",
      "it's called \"To Hell.\"\n",
      "\"It's called 'go to heaven.'\"\n",
      "===\n",
      "give us the shot!\n",
      "Give us the shard!\n",
      "\"Give us the injection!\"\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for ref, trn in zipped_test:\n",
    "    print(ref)\n",
    "    print(trn)\n",
    "    llm_output = parse_llm_output(fs_llm_chain.run(few_shot_prompt_template.format(reference=ref)))\n",
    "    print(llm_output)\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate outputs for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
