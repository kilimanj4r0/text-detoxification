{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Transformer-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reproduce fine-tuning of T5 model as authors of given paper did in [their work](https://huggingface.co/s-nlp/t5-paranmt-detox). I am planning to try different T5 model sizes for fine-tuning. Inspired by [the fine-tuning Translation example from HuggingFace](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb).\n",
    "\n",
    "Let's start with `t5-base` model as authors of the given paper did (they took even t5-base fine-tuned on pharaphrasing already). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\n",
    "import torch\n",
    "\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "tokenizer_checkpoint_path = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_checkpoint_path)\n",
    "\n",
    "train_df = pd.read_csv('../data/interim/train.csv', index_col=0)\n",
    "val_df = pd.read_csv('../data/interim/val.csv', index_col=0)\n",
    "test_df = pd.read_csv('../data/interim/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"detoxify:\"\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples[\"reference\"]]\n",
    "    targets = [ex for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build HuggingFace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b075067e478f4c8aad62db36fbfd7ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/506998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f312fd0a9e542b08677cbd5a47c6c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5788d65bf542c58d21cfc8ddaaf525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "ds = DatasetDict()\n",
    "ds['train'] = train_ds\n",
    "ds['val'] = val_ds\n",
    "ds['test'] = test_ds\n",
    "ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b446c5c91be4800be5f7b9624b00b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/506998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1656eeb296e42b28dccb8533b6914c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/56334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8eb70ab4a94f52a1cb4126f931b71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_ds_dir = '../data/interim/dshf'\n",
    "ds.save_to_disk(hf_ds_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "hf_ds_dir = '../data/interim/dshf/'\n",
    "ds = load_from_disk(hf_ds_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-06 00:09:06,167] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "model_name = 't5-base'\n",
    "batch_size = 64\n",
    "training_model_name = f\"{model_name}-finetuned-paranmt500k-detox\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f'../models/{training_model_name}',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['val'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmakharev\u001b[0m (\u001b[33mlora_yandex_sota\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/vladimir/iu/text-detoxification/notebooks/wandb/run-20231106_000951-5pjwqmrl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lora_yandex_sota/huggingface/runs/5pjwqmrl' target=\"_blank\">rose-dew-98</a></strong> to <a href='https://wandb.ai/lora_yandex_sota/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lora_yandex_sota/huggingface' target=\"_blank\">https://wandb.ai/lora_yandex_sota/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lora_yandex_sota/huggingface/runs/5pjwqmrl' target=\"_blank\">https://wandb.ai/lora_yandex_sota/huggingface/runs/5pjwqmrl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2642' max='2641' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2641/2641 36:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 04:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22414553227d/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/trainer.py:1944\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1941\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1944\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1946\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1947\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1948\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/trainer.py:2256\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2254\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2259\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:159\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m gen_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gen_kwargs \u001b[39m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mevaluate(eval_dataset, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix)\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/trainer.py:2972\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2969\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2971\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2972\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2973\u001b[0m     eval_dataloader,\n\u001b[1;32m   2974\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2975\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2976\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2977\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2978\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2979\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2980\u001b[0m )\n\u001b[1;32m   2982\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/trainer.py:3261\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3257\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3258\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3259\u001b[0m         )\n\u001b[1;32m   3260\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3261\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32m/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22414553227d/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m decoded_preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(preds, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22414553227d/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(labels \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m, labels, tokenizer\u001b[39m.\u001b[39mpad_token_id)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22414553227d/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m decoded_labels \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_decode(labels, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22414553227d/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m decoded_preds, decoded_labels \u001b[39m=\u001b[39m postprocess_text(decoded_preds, decoded_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22414553227d/home/user/vladimir/iu/text-detoxification/notebooks/2.1-transformer-models.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m result \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mdecoded_preds, references\u001b[39m=\u001b[39mdecoded_labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3510\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[1;32m   3487\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3488\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3491\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   3493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3494\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3495\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3508\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3510\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   3511\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[1;32m   3512\u001b[0m             seq,\n\u001b[1;32m   3513\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3514\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3515\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3516\u001b[0m         )\n\u001b[1;32m   3517\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[1;32m   3518\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3511\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[1;32m   3487\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3488\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3491\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m   3493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3494\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3495\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3508\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m-> 3511\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m   3512\u001b[0m             seq,\n\u001b[1;32m   3513\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3514\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3515\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3516\u001b[0m         )\n\u001b[1;32m   3517\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[1;32m   3518\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3550\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3547\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3548\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3550\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3551\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3552\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3553\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3554\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3555\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils.py:938\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    935\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    936\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 938\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    940\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    942\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    943\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils.py:914\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[1;32m    913\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index)\n\u001b[0;32m--> 914\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_ids:\n\u001b[1;32m    915\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1317\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[39m`List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m all_toks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens\n\u001b[0;32m-> 1317\u001b[0m all_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(all_toks)\n\u001b[1;32m   1318\u001b[0m \u001b[39mreturn\u001b[39;00m all_ids\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils.py:586\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    584\u001b[0m ids \u001b[39m=\u001b[39m []\n\u001b[1;32m    585\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[0;32m--> 586\u001b[0m     ids\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    587\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/transformers/tokenization_utils.py:595\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder:\n\u001b[1;32m    594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m--> 595\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id(token)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I have got only intermediate checkpoint (but it is very close in step to the final one). I will use it to make inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "tokenizer_name = 't5-base'\n",
    "trained_model_name = f\"../models/{tokenizer_name}-finetuned-paranmt500k-detox/checkpoint-2500\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(trained_model_name)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def detoxify_inference(sentences, prefix=\"detoxify:\", top_k=120, max_length=256):\n",
    "    outputs = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        text = prefix + sentence + \" </s>\"\n",
    "\n",
    "        encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        model_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k,\n",
    "            top_p=0.98,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        for output in model_output:\n",
    "            generated_sent = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            outputs.append(generated_sent)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57809</th>\n",
       "      <td>call out your butchers and I'll tell you.</td>\n",
       "      <td>Listen, call off the butchers, and I'll tell you.</td>\n",
       "      <td>0.761781</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.968063</td>\n",
       "      <td>0.005592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       reference  \\\n",
       "57809  call out your butchers and I'll tell you.   \n",
       "\n",
       "                                             translation  similarity  \\\n",
       "57809  Listen, call off the butchers, and I'll tell you.    0.761781   \n",
       "\n",
       "       lenght_diff   ref_tox   trn_tox  \n",
       "57809         0.16  0.968063  0.005592  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# The data should be downloaded and preprocessed, use 1.0-download-raw-data.ipynb and 1.2-data-preprocessing.ipynb notebooks\n",
    "test_data_path = '../data/interim/test.csv'\n",
    "test_df = pd.read_csv(test_data_path, index_col=0)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49901</th>\n",
       "      <td>One way to friggin' Mexico.</td>\n",
       "      <td>straight to Mexico.</td>\n",
       "      <td>0.621713</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.973398</td>\n",
       "      <td>0.002661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491891</th>\n",
       "      <td>You know, it pains me to find out shit about y...</td>\n",
       "      <td>you know, it also hurts me to learn things abo...</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.998989</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259555</th>\n",
       "      <td>This kid is like a boar.</td>\n",
       "      <td>this guy is like a stud.</td>\n",
       "      <td>0.652906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991821</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187825</th>\n",
       "      <td>James. What are you doing to me with your big,...</td>\n",
       "      <td>what are you doing to me!... your big, big, bi...</td>\n",
       "      <td>0.838453</td>\n",
       "      <td>0.163934</td>\n",
       "      <td>0.978909</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330314</th>\n",
       "      <td>deep down, you want someone to suck your beaut...</td>\n",
       "      <td>Deep down, you long to have your perfect feath...</td>\n",
       "      <td>0.688693</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.998196</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "49901                         One way to friggin' Mexico.   \n",
       "491891  You know, it pains me to find out shit about y...   \n",
       "259555                           This kid is like a boar.   \n",
       "187825  James. What are you doing to me with your big,...   \n",
       "330314  deep down, you want someone to suck your beaut...   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "49901                                 straight to Mexico.    0.621713   \n",
       "491891  you know, it also hurts me to learn things abo...    0.797427   \n",
       "259555                           this guy is like a stud.    0.652906   \n",
       "187825  what are you doing to me!... your big, big, bi...    0.838453   \n",
       "330314  Deep down, you long to have your perfect feath...    0.688693   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "49901      0.285714  0.973398  0.002661  \n",
       "491891     0.018182  0.998989  0.000465  \n",
       "259555     0.000000  0.991821  0.001623  \n",
       "187825     0.163934  0.978909  0.002112  \n",
       "330314     0.032787  0.998196  0.000141  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 25\n",
    "sampled_test = test_df.sample(n=n_samples, random_state=111)\n",
    "sampled_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 25/25 [00:04<00:00,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "predicitons = detoxify_inference(sampled_test['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: One way to friggin' Mexico.\n",
      "GOLD: straight to Mexico.\n",
      "PRED: that's the only way to Mexico.\n",
      "------------------------\n",
      "INIT: You know, it pains me to find out shit about you, too.\n",
      "GOLD: you know, it also hurts me to learn things about you.\n",
      "PRED: you know, I have to find out things about you, as well.\n",
      "------------------------\n",
      "INIT: This kid is like a boar.\n",
      "GOLD: this guy is like a stud.\n",
      "PRED: this baby is like a horn.\n",
      "------------------------\n",
      "INIT: James. What are you doing to me with your big, fat, hard...?\n",
      "GOLD: what are you doing to me!... your big, big, big...\n",
      "PRED: James, what are you doing to me, big, deep,...?\n",
      "------------------------\n",
      "INIT: deep down, you want someone to suck your beautiful feathers.\n",
      "GOLD: Deep down, you long to have your perfect feathers ruffled.\n",
      "PRED: Deep down, do you want another nice friend to suck on your feathers.\n",
      "------------------------\n",
      "INIT: yeah, but if it rains, we're fucked.\n",
      "GOLD: Yes, but if it rains, we're buggered.\n",
      "PRED: Actually, but if it rains, we're left.\n",
      "------------------------\n",
      "INIT: my wife sees me as a crook.\n",
      "GOLD: Wife sees me as nothing.\n",
      "PRED: My wife sees me as a cherub.\n",
      "------------------------\n",
      "INIT: Back in our day, you weren't a real man until you'd fucked one girl from each of the Seven Kingdoms and the Riverlands. - We used to call it \"making the eight.\"\n",
      "GOLD: you were not a man in our time, until you saved one girl from each of the Seven Kingdoms and Rekotos.\n",
      "PRED: the last time you were as real as you were, you suckered a girls from the Seven Kingdoms and the Riverlands.\n",
      "------------------------\n",
      "INIT: They fuckin' used you.\n",
      "GOLD: they took advantage of you!\n",
      "PRED: they used you.\n",
      "------------------------\n",
      "INIT: the fact is, if I drop my guts, and I'd look in there, I'd find the shit piled up like twigs.\n",
      "GOLD: Matter of fact, I could set my watch by my bowels, Open my gut now, they'd find turds stacked up like little black twigs,\n",
      "PRED: The fact is that on my small chin would I think of it like this and I'd find alot of knots behind me.\n",
      "------------------------\n",
      "INIT: you're Selkirk McCoy, and we fucking know it.\n",
      "GOLD: You're Selkirk McCoy, and we damn well know it.\n",
      "PRED: You're Selkirk McCoy, and we don't know it.\n",
      "------------------------\n",
      "INIT: Only after you fed them some Bullshit line about the marvels Of plastic surgery.\n",
      "GOLD: only because you told them what kind of miracles plastic surgery they can do.\n",
      "PRED: only after you gave them your take on the wonders of plastic surgery.\n",
      "------------------------\n",
      "INIT: Not pleasant, but I had smelled far worse down by the lake near that damned birch tree.\n",
      "GOLD: it wasn't pleasant, but I felt a much worse smell by the lake near that accursed belly.\n",
      "PRED: but I had been far worse by the lake, near that birch tree.\n",
      "------------------------\n",
      "INIT: I got a video of your dirty case.\n",
      "GOLD: Got the video on your ludeness case.\n",
      "PRED: I just got a video of your dirty case.\n",
      "------------------------\n",
      "INIT: What kind of crappy fire is this?\n",
      "GOLD: what's with the pore fire?\n",
      "PRED: what kind of fire is this?\n",
      "------------------------\n",
      "INIT: she belongs behind a desk where she's not going to shit in what we do on the street.\n",
      "GOLD: She belongs behind a desk, not messing with what we do on the street\n",
      "PRED: She belongs behind a desk where she wouldn't do what we do on the street.\n",
      "------------------------\n",
      "INIT: She learned everything about sex from a manual.\n",
      "GOLD: everything about sex was learned by manual.\n",
      "PRED: she learned everything about sexuality from a manual.\n",
      "------------------------\n",
      "INIT: \"Tommy, the moment I first saw Angela's eyes, I knew it. \"I knew it was love, I knew I was fucked.\n",
      "GOLD: \"Tommy, when I first saw the eyes of Angela, I knew it was love.\n",
      "PRED: \"Tommy, the moment I first saw Angela's eyes, I knew it was love, I knew it was funny.\n",
      "------------------------\n",
      "INIT: What about \"Shitcunt\"?\n",
      "GOLD: how about \"serekunda\"?\n",
      "PRED: what about the Shitcunt?\n",
      "------------------------\n",
      "INIT: He never knocked on the damn door.\n",
      "GOLD: he didn't even knock on the door.\n",
      "PRED: he never knocked on the door.\n",
      "------------------------\n",
      "INIT: I killed the bastard!\n",
      "GOLD: I killed my mother's boyfriend!\n",
      "PRED: I threw it out!\n",
      "------------------------\n",
      "INIT: holy crap, Harry!\n",
      "GOLD: Jesus wept, Harry!\n",
      "PRED: Holy crap Harry!\n",
      "------------------------\n",
      "INIT: I said, fucking right!\n",
      "GOLD: I said right!\n",
      "PRED: I said, I was going crazy!\n",
      "------------------------\n",
      "INIT: God, he's going to be a butcher.\n",
      "GOLD: Dear God, we could be looking for a butcher after all.\n",
      "PRED: God, he'll be a baron.\n",
      "------------------------\n",
      "INIT: My mom's just being silly.\n",
      "GOLD: my mom is just overreacting.\n",
      "PRED: I'm just doing it.\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for ref, trn, pred in zip(sampled_test['reference'], sampled_test['translation'], predicitons):\n",
    "    print(f'INIT: {ref}')\n",
    "    print(f'GOLD: {trn}')\n",
    "    print(f'PRED: {pred}')\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 14445/14445 [40:49<00:00,  5.90it/s] \n"
     ]
    }
   ],
   "source": [
    "refs = test_df['reference']\n",
    "preds = detoxify_inference(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14445"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame({'inputs': test_df['translation'][:len(preds)], 'preds': preds})\n",
    "\n",
    "if not os.path.exists('../data/interim/model-outputs'):\n",
    "    os.makedirs('../data/interim/model-outputs')\n",
    "\n",
    "save_result_path = f'../data/interim/model-outputs/t5-base-detox.csv'\n",
    "result_df.to_csv(save_result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
